---
title: "Gaffke's confidence bound"
header-includes:  -\newcommand{\bs}{\boldsymbol}
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

This notebook discusses an as-yet-unproven method for conservative, finite-sample, nonparametric inference on the population mean when random variables are drawn from an arbitrary bounded distribution. These bounds thus provide robust solutions to the one-sample and two-sample problems under the mild and often realistic assumption of boundedness. The usual solution of using Student's $t$-test has no finite-sample performance gaurantees for non-normal distributions, and is especially bad in practice for skewed distributions.

## Confidence bounds and sequences

We have an IID sample of $n$ bounded random variables such that $Y_i \in [0,1]$. These can be stacked into a random vector $\bs{Y} = [Y_1,...,Y_n]^T$. The unknown distribution of $Y_i$ is $F$ with mean $\mu(F) \equiv \mathbb{E}[Y_1]$. We now that $F \in \mathcal{F}_{[0,1]}$, the set of all distributions supported on $[0,1]$. 

We want to test the hypothesis $H_0: \mu(F) \leq \mu_0$ vs $H_1: \mu(F) > \mu_0$. Consider a test represented by the critical function $\phi(\bs{Y})$ that is equal to 1 if we reject $H_0$ and 0 otherwise. The probability of rejection when $Y_i \sim F$ is $\mathbb{E}_F[\phi(\bs{Y})]$, and we seek a test $\phi$ so that: 
$$\mathbb{E}_F[\phi(\bs{Y})] \leq \alpha ~~\mbox{for all}~~ F \in \mathcal{F}_{[0,1]} ~~\mbox{with}~~ \mu(F) \leq \mu_0$$
Note that, given such a test, we can exploit the duality of tests and confidence bounds to construct a one sided bound $C_l(\bs{Y})$ so that 
$$\mathbb{P}_F(C_l(\bs{Y}) \leq \mu(F) ) \geq 1-\alpha ~~\mbox{for all}~~ F \in \mathcal{F}_{[0,1]} ~~\mbox{with}~~ \mu(F) \leq \mu_0.$$
Vice versa, we can invert a level $(1-\alpha)$ confidence bound to get a level $\alpha$ test. We can also get two-sided tests and confidence intervals by constructing two one-sided intervals, one from the original samples $Y_i$ and one from the reflections $1-Y_i$ and $1-\mu_0$, since $\mathbb{E}[1-Y_i] \leq 1-\mu_0 \implies \mathbb{E}[Y_i] \geq \mu_0$. 


## Other bounds

A number of bounds for RVs supported on $[0,1]$ have already been proposed. An incomplete list includes:

**Fixed sample bounds**:

- [Hoeffding's bound](www.jstor.org/stable/2282952) for sub-Gaussians 
- [Empirical Bernstein bounds](http://arxiv.org/abs/0907.3740), which adapt to the variance of $F$
- [The Stringer bound](https://www.jstor.org/stable/1403650), widely used in auditing
- [Penny sampling](https://doi.org/10.1093/jssam/smv025), also used in auditing
- [Anderson's bound](https://apps.dtic.mil/sti/citations/AD0696676) derived from bounds on the ECDF
- [Romano and Wolf's bound](https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-3/Finite-sample-nonparametric-inference-and-large-sample-efficiency/10.1214/aos/1015951997.full) also derived from a bound on the ECDF and shown to be asympotically efficient.

**Sequentially valid bounds**:

- [Exponential supermartingales](http://arxiv.org/abs/1810.08240), which hold for a broader class of distributions than bounded $F$ and include the well-known sequential likelihood ratio tests (e.g. Wald's SPRT).
- [Betting martingales](https://arxiv.org/pdf/2010.09686.pdf), generally sharper than exponential supermartingales and related to [empirical](https://doi.org/10.1093/biomet/75.2.237) / [nonparametric](https://www.sciencedirect.com/science/article/pii/S0378375801002944) likelihood ratios.



## Gaffke's statistic

In an obscure, [2004 paper](https://www.math.uni-magdeburg.de/institute/imst/ag_gaffke/files/pp1304.pdf), Norbert Gaffke proposed a fairly simple bound. Suppose we have $\bs{D}^T = [D_1,...,D_n] \sim \mbox{Dirichlet(1,...,1)}$, a uniform draw from the $n$-dimensional simplex, and let $Q(G, \alpha)$ be the function returning the $\alpha$-quantile of distribution $G$. Denote a given (fixed) observation of $Y_i$ as $y_i$ and consider the function:
\begin{align} 
G(\bs{y}) \equiv \mathbb{P}_{\bs{D}}\left (\sum_{i=1}^{n+1} y_i D_i \leq \mu_0 \right )
\end{align}
where $y_{n+1} = 1$. Note that $\sum_{i=1}^n y_i D_i$ is exactly equal to the Bayesian bootstrap of the sample mean, as suggested by [Rubin, 1981](https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-1/The-Bayesian-Bootstrap/10.1214/aos/1176345338.full). $G(\bs{y})$ simply augments the random sample with a single point equal to the upper bound of the support, runs the Bayesian bootstrap, and takes the lower tail of the Bootstrap distribution. The proposed test is

$$\phi(\bs{y}) = \begin{cases} 1 & \mbox{if}~ G(\bs{y}) < \alpha \\ 0 & \mbox{if}~ G(\bs{y}) \geq \alpha \end{cases}$$
If it holds that 
$$\mathbb{E}_F[\phi(\bs{Y})] \leq \alpha ~~\mbox{for all}~~ F \in \mathcal{F}_{[0,1]} ~~\mbox{with}~~ \mu(F) \leq \mu_0$$
then $G(\bs{y})$ is a valid p-value for the test.  This is conjectured by both LMT and Gaffke, but remains an open question. 

## Computation

$G(\bs{y})$ is very straightforward to compute using Monte Carlo: draw $\{\bs{D}_1,\bs{D}_2,...,\bs{D}_B\}$ from the Dirichlet distribution to get $\{G_1(\bs{y}), G_2(\bs{y}),...,G_B(\bs{y})\}$ and compute the tail probability:
$$\tilde{G}(\bs{y}) = \frac{1}{B} \sum_{b=1}^B 1\{ G_b(\bs{Y}) \leq \mu_0 \}.$$

## Gaffke is exact for binary populations

If we knew that $Y_i \sim \mbox{Bernoulli}(\mu)$, then the exact uniformly most powerful test of $H_0: \mu \leq \mu_0$ is given by the lower tail of the binomial distribution: 
$$\sum_{i=0}^{k} {n \choose j} \mu_0^j  (1-\mu_0)^{n-j}$$
where $k = \sum_i y_i$ is the number of 1s observed in the sample. 

Now consider the $G(\bs{y}) = \mathbb{P}_\bs{D}(\sum_{i=1}^{n+1} y_i D_i \leq \mu_0)$. The statistic on the inside is: 
$$\sum_{i=1}^{n+1} y_i D_i  = \sum_{i=1}^{k+1} D_i \sim  \mbox{Beta}(k, n + 1 - k)$$
which follows from permutation invariance of the uniform Dirichlet, the [aggregation property](https://en.wikipedia.org/wiki/Dirichlet_distribution#Properties) of Dirichlets, and the fact that marginal distributions of Dirichlets are Beta distributions. Then $G(\bs{y})$ is given by the lower tail of a $\mbox{Beta}(k+1, n - k)$ distribution. 

To see the equivalence between these $p$-values, note that the binomial tail $\sum_{i=0}^{k} {n \choose j} \mu_0^j  (1-\mu_0)^{n-j}$ is equal to the probability of drawing $n$ $\mbox{Uniform}[0,1]$ random variables, and observing $k$ or fewer below $\mu_0$ with the rest above $\mu_0$. In other words, the probability of drawing $n$ standard uniforms and observing the $k$th order statistic to be below $\mu_0$. It is well known that the $k$th uniform order statistic has distribution:
$$U_{(k)} \sim \mbox{Beta}(k, n+1-k).$$
Thus, for binary data the UMP test is $\mathbb{P}(U_{(k)} \leq \mu_0) = G(\bs{y})$. By inverting the test, i.e., taking the $(1-\alpha)$ quantile of $U_{(k)}$ we recover the classic Clopper-Pearson bound for a binomial proportion.

The upshot of this equivalence is that $G(\bs{y})$ is not only conservative for binary $Y_i$, but also the sharpest possible conservative $p$-value. Furthermore, it is not necessary to know that the population distribution is binary to utilize this advantage, though this knowledge would certainly speed computation. $G(\bs{y})$ can be much sharper then the binomial exact test when the population distribution is supported on the interior.


## Gaffke is more powerful than betting martingales

Gaffke *does* show that $G(\bs{y})$ is always less than the p-value given by any betting martingale. This follows from a fairly straightforward geometric argument. A betting martingale p-value is never less than:
$$B(\bs{y}) = \min_{\lambda \in [\frac{-1}{1-\mu_0},\frac{1}{\mu_0}]} \prod_{i=1}^n [1 - \lambda(y_i - \mu_0)]^{-1}.$$
We aim to show that $G(\bs{y}) \leq B(\bs{y})$. Note that
$$G(\bs{y}) = \mathbb{P}_{\bs{D}}\left (\sum_{i=1}^{n+1} y_i D_i \leq \mu_0 \right ) = n! \mbox{vol}(S_n)$$
where
\begin{align}
S_n &\equiv \left \{\bs{d} \in \mathbb{R}^n : d_i \geq 0, \sum_i d_i = 1, \sum_i d_i y_i \leq \mu_0 \right \}\\
&\subset \left \{\bs{d} \in \mathbb{R}^n : d_i \geq 0, \sum_i d_i \leq 1, \sum_i\frac{d_i y_i}{\mu_0} \leq 1 \right \}\\
&= \left \{\bs{d} \in \mathbb{R}^n : d_i \geq 0, \max \left \{ \sum_i d_i, \sum_i \frac{d_i y_i}{\mu_0} \right \} \leq 1 \right \}\\
&\subset \left \{\bs{d} \in \mathbb{R}^n : d_i \geq 0, (1-\lambda) \sum_i d_i + \lambda \sum_i \frac{d_i y_i}{\mu_0}  \leq 1 \right \}\\
&= \left \{\bs{d} \in \mathbb{R}^n : d_i \geq 0, \sum_{i=1} (1 - \lambda + \lambda \frac{y_i}{\mu_0}) d_i \leq 1 \right \}
\end{align}
The volume of this final set is 

$$\frac{1}{n!} \prod_{i=1}^n (1 - \lambda + \lambda \frac{y_i}{\mu_0})^{-1} = \frac{1}{n!} \prod_{i=1}^n [1 - \lambda (1 +  \frac{y_i}{\mu_0})]^{-1} = \frac{1}{n!} \prod_{i=1}^n [1 - \tilde{\lambda} (y_i - \mu_0)]^{-1}$$
where $\tilde{\lambda} = - \lambda / \mu_0$. [**Note: I think there is a small error somewhere in here...**]

## Gaffke is efficient

[Romano and Wolf](https://projecteuclid.org/journals/annals-of-statistics/volume-28/issue-3/Finite-sample-nonparametric-inference-and-large-sample-efficiency/10.1214/aos/1015951997.full) searched succesfully for a bound that is both conservative and asymptotically efficient when $Y_i \in [0,1]$. Efficiency in this sense means that the half-width of the confidence interval, $W_n$, converges to that of the t-test: 
$$W_n \rightarrow z_{1-\alpha/2} \sigma(F) / \sqrt{n},$$
where $\sigma(F)$ is the standard deviation of $F$, which may be substantially smaller than the extremal SD of 1/2 for $Y_i \sim \mbox{Bernoulli}(0.5)$. 

The Gaffke bound is efficient in this sense. We can write 
$$\sum_{i=1}^{n+1} y_i D_i = D_{n+1} + \sum_{i=1}^n y_i D_i.$$
Since $D_{n+1} \sim \mbox{Beta}(1, n-1)$, using Chebyshev's inequality we have 
$$\mathbb{P}(\sqrt{n} D_{n+1} > \epsilon) \leq \epsilon^{-2} {\mathbb{V}[\sqrt{n} D_i]} = n \epsilon^{-2} {\mathbb{V}[D_i]} =  \epsilon^{-2} \frac{n (n-1)}{n^2 (n+1)} = \epsilon^{-2} \frac{n-1}{n(n+1)}.$$
So that $\lim_{n\rightarrow\infty} \mathbb{P}(\sqrt{n} D_{n+1} > \epsilon) = 0$, i.e. $D_{n+1} = o_p(\sqrt{n})$. Thus, to order $o_p(\sqrt{n})$, $\sum_{i=1}^{n+1} y_i D_i$ has the same distribution as the Bayesian bootstrap, which [Lo (1987)](https://projecteuclid.org/journals/annals-of-statistics/volume-15/issue-1/A-Large-Sample-Study-of-the-Bayesian-Bootstrap/10.1214/aos/1176350271.full) shows has the same distribution as the standard bootstrap. The quantile of the standard bootstrap of the sample mean then converges to $z_{1-\alpha/2} \sigma(F) / \sqrt{n}$ (see e.g. page 652 of [Lehman and Romano, 2005](https://www.springer.com/gp/book/9780387988641)). In summary, the width of the Gaffke confidence interval is: 
$$z_{1-\alpha/2} \sigma(F) / \sqrt{n} + o_p(n^{-1/2}),$$
which is the best possible. By duality, a hypothesis test based on Gaffke has a similar efficiency property: it has the same limiting power as the t-test, which is optimal.

## Gaffke is a valid p-value

This is as-yet unproven, but the empirical results are encouraging. It is clearly valid for binary populations given the equivalence to Clopper-Pearson, but the level over all $F$ on $[0,1]$ is unproven. There is work being done by [My Phan and colleagues](https://arxiv.org/abs/2106.03163) at UMass Amherst CS. I hope to help with this too.

## Simulations


```{r, eval = TRUE}

B <- 200
y <- c(rnorm(99, mean = .1, sd = .01), 1)
y <- y - mean(y) + .1
hist(y, breaks = 30)
n_grid <- seq(5, 100, by = 5)

gaffke_level <- rep(NA, length(n_grid))
t_level <- rep(NA, length(n_grid))

for(j in 1:length(n_grid)){
  p_ttest <- rep(NA, 200)
  p_gaffke <- rep(NA, 200)
  for(i in 1:200){
    Y <- sample(y, size = n_grid[j])
    p_ttest[i] <- t.test(x = Y, mu = .1)$p.value
    Z <- matrix(rexp(n = B * n_grid[j]), nrow = B, ncol = n_grid[j]) 
    D <- Z / rowSums(Z)
    p_gaffke[i] <- 1/B * sum(D %*% Y <= 0.1)
  }
  t_level[j] <- mean(p_ttest < .05)
  gaffke_level[j] <- mean(p_gaffke < .05)
}

plot(x = n_grid, y = t_level, ylim = c(0,1), col = "red", type = 'l', lwd = 3, xlab = "Sample size", ylab = "Level")
points(x = n_grid, y = gaffke_level, col = "blue", type = 'l', lwd = 3)
abline(0.05, 0, lty = "dashed")
```


